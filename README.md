# ğŸ“¦ Data Warehouse Project Submission

## ğŸ“Œ Title
**Analyzing Customer Reviews of Bank Agencies in Morocco using a Modern Data Stack**

---

## ğŸ‘¨â€ğŸ’» Author

**Mohammed Dechraoui**  
Master 2 â€“ SystÃ¨mes d'Information et SystÃ¨mes Intelligents (M2SI)  
Institut National de Statistique et d'Ã‰conomie AppliquÃ©e (INSEA)  
ğŸ“§ mohammed.dechraoui@insea.ac.ma *(example)*  
ğŸ“… Academic Year: 2024â€“2025  

---

## âœ… My Contributions

This project was developed individually as part of the Data Warehouse module. All components of the pipeline were implemented, tested, and documented by me.

### ğŸ”¹ Phase 1 â€“ Data Collection
- Implemented a Python-based web scraper using **Scrapy** to extract:
  - Bank name
  - Branch location
  - Review text
  - Rating
  - Date of review
- Used **Google Maps search patterns** to locate bank branches in Morocco.
- Exported data to structured **JSON** and imported it into PostgreSQL.
- Designed and scheduled an **Apache Airflow DAG** to automate weekly extraction.

### ğŸ”¹ Phase 2 â€“ Data Cleaning & Transformation
- Used **DBT (Data Build Tool)** to:
  - Clean raw review data (remove duplicates, missing values).
  - Normalize and preprocess text (lowercasing, stop words removal).
- Developed Python scripts to:
  - Detect the language of reviews.
  - Apply **sentiment analysis** using TextBlob.
  - Extract common themes using **LDA topic modeling**.
- Logged transformations and documented DBT models.

### ğŸ”¹ Phase 3 â€“ Data Modeling (Star Schema)
- Designed and created the following tables in **PostgreSQL**:
  - `fact_reviews`
  - `dim_bank`
  - `dim_branch`
  - `dim_location`
  - `dim_sentiment`
- Used DBT to create SQL models and load data incrementally.
- Ensured referential integrity and optimized queries for BI tools.

### ğŸ”¹ Phase 4 â€“ BI & Analytics
- Built a **Looker Studio (Data Studio)** dashboard:
  - Trends of customer sentiment per bank and branch.
  - Top positive/negative themes extracted from reviews.
  - Ranking of bank branches by average sentiment.
  - Interactive filters for location, bank, and date range.

### ğŸ”¹ Phase 5 â€“ Deployment & Automation
- Integrated all steps in a single Apache Airflow workflow:
  - Daily DAG: fetch new reviews, update DBT models, refresh schema.
  - Included email alerts for DAG failures.

---

## ğŸ“ Repository Structure

ğŸ“ project-data-warehouse/
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â””â”€â”€ review_pipeline_dag.py
â”‚   â””â”€â”€ scripts/
â”‚       â”œâ”€â”€ scraper.py
â”‚       â””â”€â”€ sentiment_analysis.py
â”œâ”€â”€ .dbt/
â”‚   â”œâ”€â”€ models/
â”‚   â””â”€â”€ dbt_project.yml
â””â”€â”€ README.md

---

## ğŸ§ª Tools & Libraries Used

- **Languages**: Python, SQL
- **Data Collection**: Scrapy, Google Maps search, JSON
- **Transformation**: DBT
- **Analysis**: TextBlob,  NLTK
- **Automation**: Apache Airflow
- **Database**: PostgreSQL
- **Visualization**: Looker Studio
- **Version Control**: GitHub

---

## ğŸ“„ Deliverables Checklist

| Deliverable                        | Status     |
|------------------------------------|------------|
| Python script for data collection  | âœ… Complete |
| Airflow DAGs for automation        | âœ… Complete |
| DBT models for transformation      | âœ… Complete |
| PostgreSQL star schema             | âœ… Complete |
| Looker Studio dashboard            | âœ… Complete |
| Project documentation              | âœ… Complete |

---

## ğŸ§  Learned Skills

- Designing a modern ETL/ELT pipeline with **Airflow and DBT**
- Applying **NLP techniques** (sentiment analysis, topic modeling) to real-world unstructured data
- Modeling a data warehouse using **star schema**
- Building **interactive BI dashboards** for business users
- End-to-end project automation with scheduling and monitoring

---

## ğŸ“š Supervisor

**Professor:** Dr. [Name of Supervisor]  
Course: Data Warehouse & Business Intelligence  
INSEA â€“ Master M2SI

---

## ğŸ“Œ Remarks

This project simulates a real-world data engineering and analytics challenge in the banking sector, using customer voice data to drive insights.  
All scripts, models, dashboards, and datasets are included in this submission and stored in the GitHub repository.

